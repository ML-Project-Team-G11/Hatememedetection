{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[GitHub]simple_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BTA0GliX2ue4",
        "X1Yd_i33N8NP",
        "6cH9ILYNN_Dq",
        "rdnpDRFrOC14",
        "oZZdL-AP2yBC",
        "wzr4zSKA7RCH",
        "N8XY0ETV5oT5",
        "Z4p4nEi96Ebx",
        "9za5zpie7CwP",
        "h5MyHXnI7taf",
        "dK40cxVADqhy",
        "gcwtOdPjENb2",
        "Eyfx8b1sETX_"
      ],
      "authorship_tag": "ABX9TyNF+/72xQwIxUtFlrOjpbaK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizavelioglu/hateful_memes-hate_detectron/blob/main/notebooks/%5BGitHub%5Dsimple_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzJGBqewtJlj"
      },
      "source": [
        "This notebook is originally prepared by DrivenData ([see original post](https://www.drivendata.co/blog/hateful-memes-benchmark/)). \n",
        "\n",
        "I have applied minor changes to the code such that it is executable on Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTA0GliX2ue4"
      },
      "source": [
        "# <font color='magenta'> <b> Imports </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCmkTrnH2pua"
      },
      "source": [
        "We will use a torchvision vision model to extract features from meme images and a fasttext model to extract features from extracted text belonging to images. These language and vision features will be fused together using torch to form a multimodal hateful memes classifer. Let's go ahead and import them now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Yd_i33N8NP"
      },
      "source": [
        "### Install PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A9dj3IzNqir"
      },
      "source": [
        "# !pip install pytorch-lightning\n",
        "!pip install pytorch_lightning==0.6.0 torch==1.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cH9ILYNN_Dq"
      },
      "source": [
        "### Install fastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgLjBmp5Nq5T"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdnpDRFrOC14"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd0LxpLI2cFF"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import random\n",
        "import tarfile\n",
        "import tempfile\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import pandas_path  # Path style access for pandas\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch                    \n",
        "import torchvision\n",
        "import fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZZdL-AP2yBC"
      },
      "source": [
        "---\n",
        "# <font color='magenta'> <b> Loading the Data </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzr4zSKA7RCH"
      },
      "source": [
        "---\n",
        "## Download & Unzip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YuwHc_ABD2h-"
      },
      "source": [
        "#@markdown ---\n",
        "#@title <h1><b><font color='red'> --Action required!-- </b></font></h1> { run: \"auto\" }\n",
        "#@markdown First, please specify the download link and the `.zip` password which both can be taken from [DrivenData](https://www.drivendata.org/competitions/70/hateful-memes-phase-2/data/)\n",
        "\n",
        "\n",
        "YOUR_LINK_TO_DOWNLOAD_PHASE2_DATA = '' #@param {type:\"string\"}\n",
        "PASSWORD_OF_ZIP = '' #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21GjdPmkOMd2"
      },
      "source": [
        "!wget -O XjiOc5ycDBRRNwbhRlgH.zip --no-check-certificate --no-proxy \"$YOUR_LINK_TO_DOWNLOAD_PHASE2_DATA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BoT53-UO7ad"
      },
      "source": [
        "# Unzips the zip quietly with the help of the \"-qq\" argument\n",
        "!unzip -qq -P \"$PASSWORD_OF_ZIP\" XjiOc5ycDBRRNwbhRlgH.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqR-9YxZ3a9S"
      },
      "source": [
        "data_dir = Path.cwd().parent / \"content\" / \"data\"\n",
        "\n",
        "img_tar_path = data_dir / \"img\"\n",
        "train_path = data_dir / \"train.jsonl\"\n",
        "dev_path = data_dir / \"dev_seen.jsonl\"\n",
        "test_path = data_dir / \"test_seen.jsonl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJhgL9Ou3cIE"
      },
      "source": [
        "train_samples_frame = pd.read_json(train_path, lines=True)\n",
        "train_samples_frame.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujvBjoRG5m2c"
      },
      "source": [
        "train_samples_frame.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8XY0ETV5oT5"
      },
      "source": [
        "---\n",
        "## Exploring the Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgAdAewq5x2p"
      },
      "source": [
        "train_samples_frame.text.map(\n",
        "    lambda text: len(text.split(\" \"))\n",
        ").describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW7XgBjw6DuR"
      },
      "source": [
        "As we may have expected, the meme text isn't usually too long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4p4nEi96Ebx"
      },
      "source": [
        "---\n",
        "##  Exploring the Image Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZATAQuu6F7q"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "images = [\n",
        "    Image.open(\n",
        "        data_dir / train_samples_frame.loc[i, \"img\"]\n",
        "    ).convert(\"RGB\")\n",
        "    for i in range(5)\n",
        "]\n",
        "\n",
        "for image in images:\n",
        "    print(image.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59h8lZIz6hj4"
      },
      "source": [
        "It looks like we'll need to resize the images to form tensor minibatches appropriate for training a model. \n",
        "\n",
        "This is where we turn to the `torchvision.transforms` module. We can use its `Compose` object to perform a series of transformations. \n",
        "\n",
        "For example, here we'll `Resize` the images (this function interpolates when needed so may distort images) then convert them to PyTorch tensors using `ToTensor`. Once the images are a uniform same size, we can make a single tensor object out of them with `torch.stack` and use the `torchvision.utils.make_grid` function to easily visualize them in Matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTMEF5P465Ow"
      },
      "source": [
        "# define a callable image_transform with Compose\n",
        "image_transform = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.Resize(size=(224, 224)),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# convert the images and prepare for visualization.\n",
        "tensor_img = torch.stack(\n",
        "    [image_transform(image) for image in images]\n",
        ")\n",
        "grid = torchvision.utils.make_grid(tensor_img)\n",
        "\n",
        "# plot\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
        "plt.axis('off')\n",
        "_ = plt.imshow(grid.permute(1, 2, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9za5zpie7CwP"
      },
      "source": [
        "---\n",
        "# <font color='magenta'> <b> Building a Multimodal Model </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjewyxAc7oSW"
      },
      "source": [
        ">Now that we have a sense of how we're going to need to process the data, we can start the model building process. There are three big-picture considerations to keep in mind as we develop the model,\n",
        "\n",
        "- Dataset handling\n",
        "- Model architecture\n",
        "- Training logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5MyHXnI7taf"
      },
      "source": [
        "---\n",
        "## Creating a Multimodal Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIGA_plR9Jkf"
      },
      "source": [
        "> Our model will need to process appropriately transformed images and properly encoded text inputs separately. That means for each sample from our dataset, we'll need to be able to access \"image\" and \"text\" data independently. Lucky for us, the PyTorch Dataset class makes this pretty easy. If you haven't yet had the pleasure of working with this object, we highly reccomend the short [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "> All we're required to do to subclass a Dataset is\n",
        "\n",
        "- Define its size by overriding `__len__`\n",
        "- Define how it returns a sample by overriding `__getitem__`\n",
        "\n",
        "> We can use the Pandas DataFrame of json records as we did above with `train_samples_frame` to do both of these things and more. We can get the length of the dataset from the samples frame, use the *`img`* column to load the images, subsample our data for faster development using the Pandas sample method, and balance the training set by slicing the dataframe based on label—we can even use DrivenData's own pandas_path accessor to help validate the data!\n",
        "\n",
        "> We want the dataset to return data ready for model input, that means `torch.tensors`. So our `__getitem__` method will need to prepare\n",
        "\n",
        "- Images by applying *`image_transform`*\n",
        "- Text by applying *`text_transform`*\n",
        "\n",
        "> *`image_transform`* was introduced above, and *`text_transform`* will be the \"sentence vector\" created by our fastText model.\n",
        "\n",
        "We'll return our samples as dictionaries with keys for\n",
        "\n",
        "- \"id\", the image id\n",
        "- \"image\", the image tensor\n",
        "- \"text\", the text tensor\n",
        "- \"label\", the label if it exists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66lBNVED7FMI"
      },
      "source": [
        "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Uses jsonl data to preprocess and serve \n",
        "    dictionary of multimodal tensors for model input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path,\n",
        "        img_dir,\n",
        "        image_transform,\n",
        "        text_transform,\n",
        "        balance=False,\n",
        "        dev_limit=None,\n",
        "        random_state=0,\n",
        "    ):\n",
        "\n",
        "        self.samples_frame = pd.read_json(data_path, lines=True)\n",
        "        self.dev_limit = dev_limit\n",
        "        if balance:\n",
        "            neg = self.samples_frame[self.samples_frame.label.eq(0)]\n",
        "            pos = self.samples_frame[self.samples_frame.label.eq(1)]\n",
        "            \n",
        "            self.samples_frame = pd.concat([neg.sample(pos.shape[0], random_state=random_state),\n",
        "                                            pos])\n",
        "\n",
        "        if self.dev_limit:\n",
        "            if self.samples_frame.shape[0] > self.dev_limit:\n",
        "                self.samples_frame = self.samples_frame.sample(\n",
        "                    dev_limit, random_state=random_state\n",
        "                )\n",
        "        self.samples_frame = self.samples_frame.reset_index(drop=True)\n",
        "        \n",
        "        self.samples_frame.img = self.samples_frame.apply(\n",
        "            lambda row: (img_dir / row.img), axis=1\n",
        "        )\n",
        "\n",
        "        # https://github.com/drivendataorg/pandas-path\n",
        "        # if not self.samples_frame.img.path.exists().all():\n",
        "        #     raise FileNotFoundError\n",
        "        # if not self.samples_frame.img.path.is_file().all():\n",
        "        #     raise TypeError\n",
        "            \n",
        "        self.image_transform = image_transform\n",
        "        self.text_transform = text_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"This method is called when you do len(instance) \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        return len(self.samples_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"This method is called when you do instance[key] \n",
        "        for an instance of this class.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
        "\n",
        "        image = Image.open(\n",
        "            self.samples_frame.loc[idx, \"img\"]\n",
        "        ).convert(\"RGB\")\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        text = torch.Tensor(\n",
        "            self.text_transform.get_sentence_vector(\n",
        "                self.samples_frame.loc[idx, \"text\"]\n",
        "            )\n",
        "        ).squeeze()\n",
        "\n",
        "        if \"label\" in self.samples_frame.columns:\n",
        "            label = torch.Tensor(\n",
        "                [self.samples_frame.loc[idx, \"label\"]]\n",
        "            ).long().squeeze()\n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image\": image, \n",
        "                \"text\": text, \n",
        "                \"label\": label\n",
        "            }\n",
        "        else:\n",
        "            sample = {\n",
        "                \"id\": img_id, \n",
        "                \"image\": image, \n",
        "                \"text\": text\n",
        "            }\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IpwB0Bxrhgl"
      },
      "source": [
        "---\n",
        "## Creating a Multimodal Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaQj4_8Rstlw"
      },
      "source": [
        "If you're new to PyTorch, check out their [guide](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html) to creating custom modules. We're going to implement a design called mid-level concat fusion.\n",
        "\n",
        "![model](https://drivendata-public-assets.s3.amazonaws.com/mid-level.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbvp6-PlrlZ7"
      },
      "source": [
        "class LanguageAndVisionConcat(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        loss_fn,\n",
        "        language_module,\n",
        "        vision_module,\n",
        "        language_feature_dim,\n",
        "        vision_feature_dim,\n",
        "        fusion_output_size,\n",
        "        dropout_p,\n",
        "        \n",
        "    ):\n",
        "        super(LanguageAndVisionConcat, self).__init__()\n",
        "        self.language_module = language_module\n",
        "        self.vision_module = vision_module\n",
        "        self.fusion = torch.nn.Linear(\n",
        "            in_features=(language_feature_dim + vision_feature_dim), \n",
        "            out_features=fusion_output_size\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(\n",
        "            in_features=fusion_output_size, \n",
        "            out_features=num_classes\n",
        "        )\n",
        "        self.loss_fn = loss_fn\n",
        "        self.dropout = torch.nn.Dropout(dropout_p)\n",
        "        \n",
        "    def forward(self, text, image, label=None):\n",
        "        text_features = torch.nn.functional.relu(\n",
        "            self.language_module(text)\n",
        "        )\n",
        "        image_features = torch.nn.functional.relu(\n",
        "            self.vision_module(image)\n",
        "        )\n",
        "        combined = torch.cat(\n",
        "            [text_features, image_features], dim=1\n",
        "        )\n",
        "        fused = self.dropout(\n",
        "            torch.nn.functional.relu(\n",
        "            self.fusion(combined)\n",
        "            )\n",
        "        )\n",
        "        logits = self.fc(fused)\n",
        "        pred = torch.nn.functional.softmax(logits)\n",
        "        loss = (\n",
        "            self.loss_fn(pred, label) \n",
        "            if label is not None else label\n",
        "        )\n",
        "        return (pred, loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t-iRRx2tO1P"
      },
      "source": [
        "---\n",
        "# <font color='magenta'> <b> Training a Multimodal Model </b> </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK40cxVADqhy"
      },
      "source": [
        "---\n",
        "## Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk0e3l_rCYU9"
      },
      "source": [
        "By subclassing the PyTorch Lightning `LightningModule`, we get most of the training logic \"for free\" behind the scenes. We just have to define what a <font color='cyan'>`forward`</font> call and <font color='cyan'>`training_step`</font> are, and provide our model with a `train_dataloader`\n",
        "\n",
        "Behavior such as checkpoint saving and early stopping can be parameterized, but need not be fully implemented because Lightning handles the details. We can also add any additional methods we want, e.g., <font color='cyan'>`make_submission_frame`</font> for preparing our competition submission csv. If you're new to PyTorch Lightning, you may fine their [quick start guide](https://pytorch-lightning.readthedocs.io/en/latest/new-project.html) useful.\n",
        "\n",
        "> We're going to implement a `LightningModule` subclass called <font color='greenyellow'>`HatefulMemesModel`</font> which takes a Python `dict` of hyperparameters called `hparams` that are used to customize the instantiation. This pattern is a Lightning convention that allows us to easily load trained models for future use, as we'll see when we generate a submission to the competition.\n",
        "\n",
        "For the language and vision module definitions, see the <font color='cyan'>`_build_model`</font> method. \n",
        "\n",
        "- The language module is going to use `fasttext` embeddings as input, computed as the `text_transform` in our data generator (we'll keep the embeddings fixed for simplicity, although they are fit to our training data). The outputs of the language module will come from a trainable `Linear` layer, as a way of fine-tuning the embedding representation during training. \n",
        "- The vision module inputs will be normalized images, computed as the `image_transform` in our data generator, and the outputs will be the outputs of a ResNet model.\n",
        "\n",
        "Note: We'll also add defaults for almost all of the `hparams` referenced in our <font color='greenyellow'>`HatefulMemesModel`</font>. This will make it easier to focus on the changes you want to make while experimenting rather than needing to include a bunch a defaults. These could be included as defaults, but Lightning is easiest to use when we keep them factored into `hparams`. This is reasonable, since everything specified by `hparams` is independend of the actual modeling architecture we defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX0wtQODtQgI"
      },
      "source": [
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics import functional as FM\n",
        "\n",
        "\n",
        "# for the purposes of this post, we'll filter\n",
        "# much of the lovely logging info from our LightningModule\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "class HatefulMemesModel(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
        "            # ok, there's one for-loop but it doesn't count\n",
        "            if data_key not in hparams.keys():\n",
        "                raise KeyError(\n",
        "                    f\"{data_key} is a required hparam in this model\"\n",
        "                )\n",
        "        \n",
        "        super(HatefulMemesModel, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        \n",
        "        # assign some hparams that get used in multiple places\n",
        "        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n",
        "        self.language_feature_dim = self.hparams.get(\"language_feature_dim\", 300)\n",
        "        # balance language and vision features by default\n",
        "        self.vision_feature_dim = self.hparams.get(\"vision_feature_dim\", self.language_feature_dim)\n",
        "        self.output_path = Path(self.hparams.get(\"output_path\", \"model-outputs\"))\n",
        "        self.output_path.mkdir(exist_ok=True)\n",
        "        \n",
        "        # instantiate transforms, datasets\n",
        "        self.text_transform = self._build_text_transform()\n",
        "        self.image_transform = self._build_image_transform()\n",
        "        self.train_dataset = self._build_dataset(\"train_path\")\n",
        "        self.dev_dataset = self._build_dataset(\"dev_path\")\n",
        "        \n",
        "        # set up model and training\n",
        "        self.model = self._build_model()\n",
        "        self.trainer_params = self._get_trainer_params()\n",
        "    \n",
        "    ## Required LightningModule Methods (when validating) ##\n",
        "    \n",
        "    def forward(self, text, image, label=None):\n",
        "        return self.model(text, image, label)\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        preds, loss = self.forward(\n",
        "            text=batch[\"text\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"label\"]\n",
        "        )\n",
        "        \n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        preds, loss = self.eval().forward(\n",
        "            text=batch[\"text\"], \n",
        "            image=batch[\"image\"], \n",
        "            label=batch[\"label\"]\n",
        "        )\n",
        "\n",
        "        acc = FM.accuracy(preds, batch[\"label\"], num_classes=2)\n",
        "        \n",
        "        return {\"batch_val_loss\": loss, \"batch_val_acc\": acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_loss\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "        \n",
        "        avg_acc = torch.stack(\n",
        "            tuple(\n",
        "                output[\"batch_val_acc\"] \n",
        "                for output in outputs\n",
        "            )\n",
        "        ).mean()\n",
        "\n",
        "        return {\n",
        "            \"val_loss\": avg_loss,\n",
        "            \"progress_bar\":{\"avg_val_loss\": avg_loss, \"avg_val_acc\": avg_acc}\n",
        "        }\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizers = [\n",
        "            torch.optim.AdamW(\n",
        "                self.model.parameters(), \n",
        "                lr=self.hparams.get(\"lr\", 0.001)\n",
        "            )\n",
        "        ]\n",
        "        schedulers = [\n",
        "            torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizers[0]\n",
        "            )\n",
        "        ]\n",
        "        return optimizers, schedulers\n",
        "    \n",
        "    # @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_dataset, \n",
        "            shuffle=True, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "        )\n",
        "\n",
        "    # @pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.dev_dataset, \n",
        "            shuffle=False, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
        "        )\n",
        "    \n",
        "    ## Convenience Methods ##\n",
        "    \n",
        "    def fit(self):\n",
        "        self._set_seed(self.hparams.get(\"random_state\", 42))\n",
        "        self.trainer = pl.Trainer(**self.trainer_params)\n",
        "        self.trainer.fit(self)\n",
        "        \n",
        "    def _set_seed(self, seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    def _build_text_transform(self):\n",
        "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
        "            ft_path = Path(ft_training_data.name)\n",
        "            with ft_path.open(\"w\") as ft:\n",
        "                training_data = [\n",
        "                    json.loads(line)[\"text\"] + \"/n\" \n",
        "                    for line in open(\n",
        "                        self.hparams.get(\"train_path\")\n",
        "                    ).read().splitlines()\n",
        "                ]\n",
        "                for line in training_data:\n",
        "                    ft.write(line + \"\\n\")\n",
        "                language_transform = fasttext.train_unsupervised(\n",
        "                    str(ft_path),\n",
        "                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n",
        "                    dim=self.embedding_dim\n",
        "                )\n",
        "        return language_transform\n",
        "    \n",
        "    def _build_image_transform(self):\n",
        "        image_dim = self.hparams.get(\"image_dim\", 224)\n",
        "        image_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(\n",
        "                    size=(image_dim, image_dim)\n",
        "                ),        \n",
        "                torchvision.transforms.ToTensor(),\n",
        "                # all torchvision models expect the same\n",
        "                # normalization mean and std\n",
        "                # https://pytorch.org/docs/stable/torchvision/models.html\n",
        "                torchvision.transforms.Normalize(\n",
        "                    mean=(0.485, 0.456, 0.406), \n",
        "                    std=(0.229, 0.224, 0.225)\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        return image_transform\n",
        "\n",
        "    def _build_dataset(self, dataset_key):\n",
        "        return HatefulMemesDataset(\n",
        "            data_path=self.hparams.get(dataset_key, dataset_key),\n",
        "            img_dir=self.hparams.get(\"img_dir\"),\n",
        "            image_transform=self.image_transform,\n",
        "            text_transform=self.text_transform,\n",
        "            # limit training samples only\n",
        "            dev_limit=(\n",
        "                self.hparams.get(\"dev_limit\", None) \n",
        "                if \"train\" in str(dataset_key) else None\n",
        "            ),\n",
        "            balance=True if \"train\" in str(dataset_key) else False,\n",
        "        )\n",
        "    \n",
        "    def _build_model(self):\n",
        "        # we're going to pass the outputs of our text\n",
        "        # transform through an additional trainable layer\n",
        "        # rather than fine-tuning the transform\n",
        "        language_module = torch.nn.Linear(\n",
        "                in_features=self.embedding_dim,\n",
        "                out_features=self.language_feature_dim\n",
        "        )\n",
        "        \n",
        "        # easiest way to get features rather than\n",
        "        # classification is to overwrite last layer\n",
        "        # with an identity transformation, we'll reduce\n",
        "        # dimension using a Linear layer, resnet is 2048 out\n",
        "        vision_module = torchvision.models.resnet152(\n",
        "            pretrained=True\n",
        "        )\n",
        "        vision_module.fc = torch.nn.Linear(\n",
        "                in_features=2048,\n",
        "                out_features=self.vision_feature_dim\n",
        "        )\n",
        "\n",
        "        return LanguageAndVisionConcat(\n",
        "            num_classes=self.hparams.get(\"num_classes\", 2),\n",
        "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
        "            language_module=language_module,\n",
        "            vision_module=vision_module,\n",
        "            language_feature_dim=self.language_feature_dim,\n",
        "            vision_feature_dim=self.vision_feature_dim,\n",
        "            fusion_output_size=self.hparams.get(\n",
        "                \"fusion_output_size\", 512\n",
        "            ),\n",
        "            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n",
        "        )\n",
        "    \n",
        "    def _get_trainer_params(self):\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            filepath=self.output_path,\n",
        "            monitor=self.hparams.get(\n",
        "                \"checkpoint_monitor\", \"avg_val_acc\"\n",
        "            ),\n",
        "            mode=self.hparams.get(\n",
        "                \"checkpoint_monitor_mode\", \"max\"\n",
        "            ),\n",
        "            verbose=self.hparams.get(\"verbose\", True)\n",
        "        )\n",
        "\n",
        "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
        "            monitor=self.hparams.get(\n",
        "                \"early_stop_monitor\", \"avg_val_acc\"\n",
        "            ),\n",
        "            min_delta=self.hparams.get(\n",
        "                \"early_stop_min_delta\", 0.001\n",
        "            ),\n",
        "            patience=self.hparams.get(\n",
        "                \"early_stop_patience\", 5\n",
        "            ),\n",
        "            verbose=self.hparams.get(\"verbose\", True),\n",
        "        )\n",
        "\n",
        "        trainer_params = {\n",
        "            \"checkpoint_callback\": checkpoint_callback,\n",
        "            \"early_stop_callback\": early_stop_callback,\n",
        "            # \"default_save_path\": self.output_path,\n",
        "            \"accumulate_grad_batches\": self.hparams.get(\n",
        "                \"accumulate_grad_batches\", 1\n",
        "            ),\n",
        "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
        "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
        "            \"gradient_clip_val\": self.hparams.get(\n",
        "                \"gradient_clip_value\", 1\n",
        "            ),\n",
        "        }\n",
        "        return trainer_params\n",
        "            \n",
        "    @torch.no_grad()\n",
        "    def make_submission_frame(self, test_path):\n",
        "        test_dataset = self._build_dataset(test_path)\n",
        "        submission_frame = pd.DataFrame(\n",
        "            index=test_dataset.samples_frame.id,\n",
        "            columns=[\"proba\", \"label\"]\n",
        "        )\n",
        "        test_dataloader = torch.utils.data.DataLoader(\n",
        "            test_dataset, \n",
        "            shuffle=False, \n",
        "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
        "            num_workers=self.hparams.get(\"num_workers\", 16))\n",
        "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
        "            preds, _ = self.model.eval().to(\"cpu\")(\n",
        "                batch[\"text\"], batch[\"image\"]\n",
        "            )\n",
        "            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
        "            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
        "        submission_frame.proba = submission_frame.proba.astype(float)\n",
        "        submission_frame.label = submission_frame.label.astype(int)\n",
        "        return submission_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG_hT_QXDvX7"
      },
      "source": [
        "Let's recap what we've done. We've separated our data processing, modeling, and training logic:\n",
        "- Data processing code is contained inside of <font color='greenyellow'>`HatefulMemesDataset`</font>, which subclasses PyTorch `Dataset`\n",
        "- Multimodal fusion model code is contained inside of <font color='greenyellow'>`LanguageAndVisionConcat`</font>, which subclasses PyTorch `torch.nn.Module`\n",
        "- Training, early stopping, checkpoint saving, and submission building code is contained inside of <font color='greenyellow'>`HatefulMemesModel`</font>, which subclasses the PyTorch Lightning `pl.LightningModule`\n",
        "\n",
        "> A <font color='greenyellow'>`HatefulMemesModel`</font> can be instantiated using only a `dict` of `hparams`. There are only a few required hparams—the paths which point to our `.jsonl` files as well as the image directory. Our `__init__` will tell us if we've forgotten those. Beyond that, there are many hyperparameters we could specifiy in order to experiment with different models and early stopping strategies, batch sizes, learning rates, ..., but thanks to the handy `.get` method on Python dictionaries, our code won't fail us if we fail to specify these parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcwtOdPjENb2"
      },
      "source": [
        "---\n",
        "## Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YULv0-qEO1-"
      },
      "source": [
        "hparams = {\n",
        "    \n",
        "    # Required hparams\n",
        "    \"train_path\": train_path,\n",
        "    \"dev_path\": dev_path,\n",
        "    \"img_dir\": data_dir,\n",
        "    \n",
        "    # Optional hparams\n",
        "    \"embedding_dim\": 150,\n",
        "    \"language_feature_dim\": 300,\n",
        "    \"vision_feature_dim\": 300,\n",
        "    \"fusion_output_size\": 256,\n",
        "    \"output_path\": \"model-outputs\",\n",
        "    \"dev_limit\": None,\n",
        "    \"lr\": 0.0005,\n",
        "    \"max_epochs\": 10,\n",
        "    \"n_gpu\": 1,\n",
        "    \"batch_size\": 4,\n",
        "    # allows us to \"simulate\" having larger batches \n",
        "    \"accumulate_grad_batches\": 16,\n",
        "    \"early_stop_patience\": 3,\n",
        "}\n",
        "\n",
        "hateful_memes_model = HatefulMemesModel(hparams=hparams)\n",
        "hateful_memes_model.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Aymg9hzbA_P"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyfx8b1sETX_"
      },
      "source": [
        "---\n",
        "# <font color='magenta'> <b> Making a submission </b> </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B0upKS_EVOo"
      },
      "source": [
        "# we should only have saved the best checkpoint\n",
        "checkpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\n",
        "assert len(checkpoints) == 1\n",
        "\n",
        "checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXM9OaY9EYwn"
      },
      "source": [
        "hateful_memes_model = HatefulMemesModel.load_from_checkpoint(\n",
        "    checkpoints[0]\n",
        ")\n",
        "submission = hateful_memes_model.make_submission_frame(\n",
        "    test_path\n",
        ")\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHBcUYjBEbsu"
      },
      "source": [
        "# Since this is a first pass, let's check a couple of things.\n",
        "submission.groupby(\"label\").proba.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bddpkGlVEero"
      },
      "source": [
        "submission.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmLEmLrEjwP"
      },
      "source": [
        "submission.to_csv((\"model-outputs/submission.csv\"), index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}